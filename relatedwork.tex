%interessant om tabel te maken: https://www.w3.org/wiki/RdfStoreBenchmarking en om bibliografie aan te vullen?
\begin{table*}[t!]
	\centering
	\caption{Overview of recent (2011-2016) benchmarking results. Virtuoso is abbreviated as Virt and GraphDB as Gra.}
	\label{benchmarks}
	\scalebox{0.85}{
	\begin{tabular}{l|l|l|l|l|p{3cm}}
		\hline
		\textbf{Benchmark/Paper}                                                                                             & \textbf{Year} & \textbf{Dataset(s)}                                                  & \textbf{Triple Stores}                                                                                                                                   & \textbf{Nodes x RAM}                          & \textbf{Remarks}                                               \\ \hline
		\begin{tabular}[c]{@{}l@{}}Graux et al.~\cite{graux2016multi}\end{tabular} & 2016          & \begin{tabular}[c]{@{}l@{}}WatDiv1k, LUBM1k, \\ LUBM10k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Standalone (CumulusRDF~\cite{ladwig2011cumulusrdf},...)\\ HDFS with prep.: S2RDF~\cite{Schatzle:2016:SRQ:2977797.2977806},...\\ HDFS no prep.: PigSPARQL~\cite{schatzle2011pigsparql},...\end{tabular} & 10 x 17GB                              &                                                               \\ \hline
		SPB~\cite{kotsevbenchmarking}                                                                                                                  & 2016          & \begin{tabular}[c]{@{}l@{}}SPB64M, SPB256M, \\ SPB1B\end{tabular}    & Virtuoso, GraphDB                                                                                                                                        & \begin{tabular}[c]{@{}l@{}}Virt(192 GB),\\ Gra(64GB)\end{tabular}              & \begin{tabular}[c]{@{}l@{}} \end{tabular}  \\ \hline
		Hernandez et al.~\cite{hernandez2016querying}                                                                                                 & 2016          & Wikidata                                                             & \begin{tabular}[c]{@{}l@{}}4store~\cite{harris20094store}, Blazegraph,\\ GraphDB, Jena TDB,\\ Virtuoso, Neo4J,\\ PostGreSQL\end{tabular}                                        & 1 x 32GB                               &                                         \\ \hline
		S2RDF~\cite{Schatzle:2016:SRQ:2977797.2977806} & 2016 &\begin{tabular}[c]{@{}l@{}}WatDiv10M,\\ WatDiv100M \end{tabular} & \begin{tabular}[c]{@{}l@{}}S2RDF~\cite{Schatzle:2016:SRQ:2977797.2977806}, H2RDF+~\cite{papailiou2013h}, Sempala~\cite{schatzle2014sempala}, \\ PigSPARQL~\cite{schatzle2011pigsparql}, SHARD~\cite{rohloff2010high}, Virtuoso\end{tabular}  & \begin{tabular}[c]{@{}l@{}}10 x 32GB, \\ Virt(1 x 32GB)\end{tabular} &  \\ \hline
		BigRDFBench~\cite{Saleem} & 2015 & 13 real datasets & \begin{tabular}[c]{@{}l@{}}FedX~\cite{DBLP:conf/semweb/SchwarteHHSS11}, SPLENDID~\cite{gorlitz2011splendid}, ANAPSID~\cite{Acosta2011}, \\ FedX+HiBISCuS~\cite{saleem2014hibiscus}, \\ SPLENDID+HiBISCuS\end{tabular} & 1 x 8GB & \begin{tabular}[c]{@{}l@{}}FedBench \\+ 18 new queries\end{tabular} \\ \hline
		FEASIBLE~\cite{saleem2015feasible} & 2015 & generator & \begin{tabular}[c]{@{}l@{}} Virtuoso7, Sesame, \\ Jena TDB, OWLIM-SE  \end{tabular} & 1 x 16GB &  \\ \hline
		%title & year & dataset & stores & hardware &  \\ \hline			
		WatDiv~\cite{alucc2014diversified}                                                                                                               & 2014          & \begin{tabular}[c]{@{}l@{}}WatDiv10M,\\ WatDiv100M \end{tabular}                                                & \begin{tabular}[c]{@{}l@{}}MonetDB~\cite{boncz2005monetdb}, RDF-3X~\cite{neumann2010rdf}, \\ Virtuoso6, Virtuoso7, \\ gStore~\cite{zou2011gstore}, 4store\end{tabular}                                                     & 1  x 16GB                              &                                                              \\ \hline
		\begin{tabular}[c]{@{}l@{}}Cudr\'e-Mauroux et al.~\cite{cudre2013nosql} \end{tabular}                                                                                          & 2013          & \begin{tabular}[c]{@{}l@{}}BSBM (10, 100, \\ 1000M) DBPSB \end{tabular}                                        & \begin{tabular}[c]{@{}l@{}}4store, Hive+HBase,\\ CumulusRDF, Couchbase,\\ Jena+HBase~\cite{khadilkar2012jena}\end{tabular}                                                       & \begin{tabular}[c]{@{}l@{}} $2^n$  x 8GB \\ $n=0,1,...4$ \end{tabular}  & 
		\begin{tabular}[c]{@{}l@{}}  \end{tabular} \\ \hline
		BioBenchmark Toyama~\cite{wu2014biobenchmark} & 2012 & \begin{tabular}[c]{@{}l@{}}5 biological datasets \\ (10M - 8000M) \\ Uniprot, DDBJ,... \end{tabular} & \begin{tabular}[c]{@{}l@{}}4store, BigData, Mulgara, \\ Virtuoso, OWLIM-SE
		\end{tabular} & 1 x 64GB & 5-20 queries per dataset \\ \hline
		FedBench~\cite{Schmidt2011} & 2011 & \begin{tabular}[c]{@{}l@{}}11 endpoints \\ with $\le 50$M\end{tabular}{} & SPLENDID, Alibaba, Sesame & 1 x 32GB & \begin{tabular}[c]{@{}l@{}}14 federated queries \\ (7 life sciences, \\ 7 cross-domain)\end{tabular} \\ \hline
	\end{tabular}
	}
\end{table*}

%EARLY BENCHMARK (generators)
%Real data!
%DBPedia SPARQL Benchmark 	(DBSPB)		Dataset=DBPedia, Queries extracted from DBPedia query logs, mostly lookup queries, no inference
%
%Artificial datasets:
%
%The Lehigh University Benchmark	(LUBM)		Scalable dataset generator, BGP queries, university domain ontology
%									SP2Bench	DBLP simple bibliographic schema, well structured data, more complex patterns and some 
	  											%SPARQL 1.0 operators, long path chains, filter, optional, bound
%Berlin SPARQL Benchmark			BSBM		ecommerce scenario, scalable data generator, no reasoning
%Semantic Publishing Benchmark		SPB			Arbitrarily large datasets (billions of triples), more complex query worlkload, all 1.0 operators, nesting, 
												%high complexity
There is an abundance of Linked Data benchmarks mainly operating on \emph{artificial} datasets, the most popular ones being (chronologically) the Lehigh University Benchmark~\cite{guo2005lubm} (LUBM), the SPARQL performance benchmark~\cite{schmidt2009sp} (SP$^2$Bench), and the Berlin SPARQL benchmark~\cite{bizer2009berlin}
(BSBM).  
%WatDiv adresses shortcomings, Feasible more complex queries
The shortcomings of these early benchmarks were addressed in recent work, which resulted in the Waterloo SPARQL Diversity Test
Suite~\cite{alucc2014diversified} (WatDiv). This new benchmark focuses on \emph{diversity} both in terms of the query properties and data properties. The first is achieved by generating queries from 20 Basic Graph Pattern (BGP) query templates with different shapes. The latter affects the triple pattern selectivity and therefore reveals the ability of the internal query planning algorithms in RDF systems to make the most efficient choice to resolve a query. 
%More complex queries
More diverse SPARQL queries can be generated through the FEASIBLE~\cite{saleem2015feasible} benchmark generator. The queries are selected by first converting them to normalized feature vectors and then choosing a set of mutually distant queries. Similarly, the Semantic Publishing Benchmark~\cite{kotsevbenchmarking} (SPB) provides complex nested queries and uses all SPARQL~1.0 operators.

%Synthetic versus Real-World: problems
% DuanKSU11: Apples and Oranges: A Comparison of RDF Benchmarks and Real RDF Datasets
%In this paper, we compare data generated with existing RDF benchmarks and data found in widely used real RDF datasets. The
%results of our comparison illustrate that existing benchmark data have little in common with real data. Therefore any conclusions
%drawn from existing benchmark tests might not actually translate to expected behaviours in real settings. In terms of the comparison
%itself, we show that simple primitive data metrics are inadequate to flesh out the fundamental differences between real and benchmark data.
A recurring criticism on synthetic benchmarks is that they have very little in common with real application domains~\cite{DBLP:conf/sigmod/DuanKSU11}. Therefore, it is not possible to generalize benchmark results of RDF databases on artificial data to real-world use cases.
%real-world benchmarks then?
%BioBenchmark Toyama 2012
%For this evaluation, we used biological databases, Cell Cycle Ontology, Allie, PDBj, UniProt, and DDBJ con- taining as many as 8 billion triples
%8 billion triples.
%We evaluated the load and query costs of five popular triple stores: 4store, Bigdata, Mulgara, Virtuoso, and OWLIM-SE. To the best of our knowledge, 
%we evaluated the largest scale of real biological data possible on a single node.
%
One of the first real-world benchmarks is the DBpedia SPARQL benchmark~\cite{morsey2011dbpedia} (DBSB) which uses mostly BGPs extracted from actual server logs.
Specifically for the Life Sciences domain, BioBenchmark Toyama 2012~\cite{wu2014biobenchmark} evaluates 5 triple stores on 5 biological datasets (Cell Cycle Ontology~\cite{antezana2009cell}, Allie~\cite{yamamoto2011allie}, PDBj~\cite{kinjo2011protein}, UniProt~\cite{uniprot2014uniprot}, and DDBJ~\cite{tateno2002dna}), ranging from 10 million to 8 billion triples.

%Multi-node + Query Correctness!!
All benchmarks mentioned so far focus on single-node RDF databases. FedBench~\cite{Schmidt2011} is a system to test query federators. They evaluate 3 federated systems using 14 real-world federated queries, of which 7 from the Life Sciences domain. In recent work, BigRDFBench~\cite{Saleem} increases the number of datasets from 11 to 13 and adds 18 new federated queries. Instead of just focusing on query runtime, other performance metrics are taken into account such as source selection and query correctness. An alternative heuristic approach for automatically generating federated queries is the SPARQL Linked Open Data Query Generator~\cite{gorlitz2012splodge} (SPLODGE).

%Niet enkel native RDF stores
%\todo{Referenties toevoegen voor elk van die systemen? }\\
The previously mentioned benchmarking efforts focus on native RDF systems. A first generalization involves adding other graph and relational databases. For example, in the WikiData benchmarking effort~\cite{hernandez2016querying}, Neo4J and PostgreSQL were added. 
A second generalization involves mapping SPARQL workloads on NoSQL and Hadoop-based systems~\cite{cudre2013nosql, graux2016multi, Schatzle:2016:SRQ:2977797.2977806}. Graux~\cite{graux2016multi} compared 3 different types of systems: (i) Standalone NoSQL-based approaches such as CumulusRDF~\cite{ladwig2011cumulusrdf} which translates queries to the Cassandra Query Language; (ii) HDFS-based (Hadoop Distributed File System~\cite{ghemawat2003google}) approaches with a data preparation phase such as S2RDF~\cite{Schatzle:2016:SRQ:2977797.2977806}; and (iii) HDFS-based approaches which natively store RDF, such as PigSPARQL~\cite{schatzle2011pigsparql}. 
%In the S2RDF research paper~\cite{Schatzle:2016:SRQ:2977797.2977806}, a comparison is made with other HDFS-based approaches and a single server instance of Virtuoso.}{den}

RDF systems are also evaluated in two European H2020 projects: LDBC~\cite{LDBC} and 
HOBBIT~\cite{HOBBIT}. Within LDBC a number of RDF benchmarks were developed~\cite{Boncz:2013:LBG:2513591.2527070}, one benchmark is based on social network data~\cite{erling2015ldbc} and SPB~\cite{kotsevbenchmarking} is based on a data publishing case with BBC. 
In the HOBBIT project a platform is being built to offer industry a unified approach for running benchmarks related to their actual workloads.

%prior work
In Table~\ref{benchmarks} we provide an overview of the most recent benchmarking results together with information on their time of release, the datasets used,
the systems tested, and the hardware setup.

\subsection{Positioning this work}

%WAT DOEN WE!

\paragraph{Scalable Approaches}
Our work distinguishes itself from other efforts by considering seven scalable approaches to handle Big Linked Data: four vendor-backed RDF databases and three research prototypes. Any system can be tested with our approach, the only requirement is support for the SPARQL protocol.

\paragraph{Comprehensive}
The benchmark is comprehensive in evaluating query performance: the dataset volume, the different hardware and engine configurations, the query properties, and the query context are all taken into account. We also explicitly analyze whether the query results are complete. 

%Life Sciences / Up-to-date  / Artificial
\paragraph{Life Sciences domain}
We provide an up-to-date review of state-of-the-art RDF storage systems both on artificial datasets as well as on Life Sciences data. 


%bm approach - reproduce
\paragraph{Reproducible}
We make it easy to reproduce our benchmark results by providing scripts and images for automatic deployment of the benchmark components as well as the notebooks for post-processing. 


%Conclusions
\paragraph{Unbiased Results}
Conclusion are drawn across multiple approaches to Linked Data querying by focusing on the financial cost instead of query runtimes.
Results are interpreted in an unbiased fashion, with the help of decision trees trained on query features.

%To facilitate reproducibility, we release our benchmark data in a rich event format. Additionally, we provide post-processing scripts in the form of Jupyter notebooks and tools that facilitate deployment and configuration of benchmark components.

%\paragraph{Up-to-date view} Specifically for the Life Sciences domain BioBenchmark Toyama 2012 is the most recent report for single-node setups.
%
%\paragraph{Scalability} We study scalability not only in terms of dataset sizes (WatDiv), but also in terms of the size of the distributed setup (horizontal scalability) and in terms of memory resources (vertical scalability).
%
%\paragraph{Broad set of query types} Where the WatDiv runs are diverse in the space of BGP queries, the queries of Ontoforce are complex, rich in SPARQL keywords, sub-queries are common and a large fraction consists of non-conjunctive queries, which are typically very challenging~\cite{Picalausa2011}. 
%
%\paragraph{Query Correctness} Just like BigRDFBench we explicitly verify query correctness before turning to runtime comparisons and demonstrate its necessity for challenging queries.
%
%%\paragraph{Objective and exhaustive} By considering different hardware and configuration setups our work becomes more objective. As an example the S2RDF paper compares Hadoop-based systems with Virtuoso and concludes a similar performance, but does not take into account that (as will be shown later), performance does not drop when adding multiple clients, thereby increasing Virtuoso's ETL throughput by an order of magnitude.
%
%\paragraph{Multi-setup} This work compares single and multi-node setups, federated querying, and compression by using benchmark cost as a unification parameter.
%
%\paragraph{Query-mix size} Whereas many benchmarks have a limited query-set, both the WatDiv and Ontoforce benchmark used in this evaluation can be considered stress tests with respectively 400 and 1,223 queries. 
%
%\paragraph{Flexibility} Any system can be tested with our approach, the only requirement is support for the SPARQL protocol. Because of this we can for example also test the Triple Pattern Fragments (TPF) system, since the TPF client can be run as an http-server.

%\subsection{Prior Results}
%\todo{Prior results moet naar related work maar WatDiv introduceren!}

\subsection{Prior work}

In our initial conference paper~\cite{de2016big} we evaluated 4 RDF databases on WatDiv~\cite{alucc2014diversified}, with 3 different dataset sizes: 10M (10 million), 100M and 1000M triples. These \emph{Vendor} systems were run as-is, without any configuration. 

Ontoforce~\cite{ontoforcewebsite} provided us with a real-world Life \mbox{Sciences} dataset used in the back-end of their product \mbox{DISQOVER~\cite{disqover}}. This proprietary data and query-set was previously analyzed in our SWAT4LS proceeding~\cite{dewitte_swat4ls_2016}.  of which we provide a summary in the section `Datasets and Queries'. 
%on page~\pageref{subsec:dataqueries}.
In this proceeding we analyzed the queries according to their SPARQL keywords and structural features~\cite{DBLP:journals/corr/abs-1103-5043}, an approach we will further extend in this work. 

%how we addressed problems in prior work
Counter-intuitive results in these conference papers motivated re-running some of the benchmarks in this work. We paid close attention to query completeness and engine configuration. For the latter we distinguish between a \emph{Documented} and an \emph{RFI-optimized} configuration (see section `Store Preselection'). 

In the current work all seven systems are evaluated on both WatDiv and the Ontoforce benchmark. \todo{(this was previously not the case!)}

%We were also confronted with counter-intuitive results in terms of runtime. This required re-running some of the tests and extending the benchmark software to further clarify these issues. All benchmarks involving Virtuoso on the Ontoforce data in our SWAT4LS contribution have therefore been duplicated.
%Originally, the Ontoforce benchmark was used to study scalable RDF approaches, here we also evaluated the other \emph{Vendor} systems. Additionally, we evaluated 3 (research) \emph{Prototypes} on both Ontoforce and WatDiv benchmarks.

%that are implementations of compression, federation, %and the Linked Data Fragments %concept~\cite{DBLP:conf/semweb/VerborghHMHVSCCMW14} on %WatDiv and Ontoforce data.
