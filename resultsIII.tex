

%waarom uitleggen
% in store preselection staat het volgende:
%The comparison with these SemWeb systems was an essential part of the research collaboration with Onto-
%force as their initial goal was to build their DISQOVER search interface on top of a federated querying system.
%The advantage of the latter is that their interface would then provide a live view on a continuously updating
%Life Sciences Linked Data cloud, removing the need for an ETL process.
As was explained in section~\ref{subsec:featurematrix}, the WatDiv benchmark and the Ontoforce benchmark are related. The Ontoforce benchmark consists of interactive federated queries which are extracted from the user logs of the DISQOVER product. These queries are currently solved by combining an ETL preprocessing step, which integrates the different Life Sciences datasets offline using Ontoforce's own central ontology. This ETL step bares a lot of similarity with the WatDiv benchmark as it consists of mostly BGP queries. The queries of the Ontoforce benchmark are the result of faceted browsing, whereby, in practice, the facet filters are performed by a distributed search system (SOLR), but their product can also run with a SPARQL-based back-end. In this section we evaluate the ability of \emph{Vendor} systems to work with these types of queries and therefore serve as an alternative to a search system.

%Runtimes - minder belangrijk, meer focus op waarom
%N1_64_Opt Ontoforce => for both ES and Virtuoso no response time <> runtime
%Blazegraph crashed
%Es 380848.276950983	380849.972397525				=> coincide
%GraphDB crashed
%23258.549703492	23261.613780134						=> coincide
The Ontoforce benchmark has a very challenging query set. Therefore the focus of section~\ref{sec:realworld} will be far less on query runtimes but more on trying to extract insights which are generalizable. In this benchmark run the response times consistently coincide with the query runtimes.  
%More challenging => meer errors => besproken in aparte sectie
In section~\ref{subsec:erroranalysis} we give more detailed insights in the behavior of the different systems on the Ontoforce benchmark. We pay special attention to query failures and query completeness.
In section~\ref{subsec:dtrees} we try to automatically extract the reasons behind query success, failure, different error types, and slow versus fast running queries. This automation is achieved by making use of decision tree analysis.
Finally, in section~\ref{sec:bmcost} we compare the results of all benchmarks in this research using \emph{Benchmark Cost} as a unification parameter. This allows to make comparisons between setups which are very different in nature and see whether the trends in the benchmark results are consistent. This approach also takes into account the cost for data ingestion and the different licensing fees.


\subsection{Benchmark Error Analysis}
\label{subsec:erroranalysis}
\input{results_realworld.tex}



\subsection{Decision Tree Analysis of Query Features}
\label{subsec:dtrees}
\input{results_features.tex}



\subsection{Benchmark Cost}
\label{sec:bmcost}


\input{results_setups.tex}



