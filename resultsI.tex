%runtimes: median - mean - response
In this section we will study the query runtime distributions of different approaches for dealing with Linked Data at scale. To summarize runtime results we rely on mean and median values. If a query is executed multiple times, the query runtime is defined as the median value of the different executions. If we aggregate runtimes over different queries, for example per query template, we report both the \emph{median} and \emph{mean query runtimes}. As the runtime distributions can be skewed, performance differences between systems are most often reported using the median runtime. 

If we consider an ETL~\cite{ETL} process, or equivalently a batch of queries, the mean runtime is more meaningful, as it directly translates to the total batch runtime. In the following box plots we chose to report both.

%response times
Some of the stores provide query results in a streaming fashion. Response times are not captured in the current query event format but are captured in the SPARQL benchmarker summary CSV-files. For GraphDB and Blazegraph the response times are respectively 27\% and 21\% lower than the mean runtimes on WatDiv1000M. For the other engines the difference was close to zero.
%N1_64_Opt WatDiv 1000M: 
%Store 			Avg Mix Response 	Avg Mix Runtime
%Blazegraph: 	7436.614781577 		9361.074445217   	=> 1925 sneller = 21% lager
%Es 			45543.319632289	    45548.207560219  	=> coincide
%GraphDB   		6095.473343756 		8377.37145798 		=> 2282 sneller = 27% lager
%Virtuoso       1984.603994124	    2021.146539186		=> coincide
% LdF1 en LDF3 											=> coincide

%errors excluded
A major concern when comparing query runtimes between different engines is \emph{query completeness}. The current query event format, shown in Table~\ref{table:queryevents}, explicitly reports whether a query was solved correctly, meaning it has retrieved the complete set of results. In the sections `Results III' query completeness is the topic of of the first subsection.
%~\ref{subsec:completeness} 
To interpret the results in this section correctly, it is important to understand that queries, which have incomplete results for at least one benchmark, are completely discarded in the runtime comparisons.

\begin{table}[ht!]
	\centering
	\caption{Conventions for describing benchmark setups.}
	\label{table:namingconv}
	\scalebox{0.9}{
		\begin{tabular}{l|l}
			\hline
			\textbf{Shorthand} & \textbf{Full Description} \\
			\textbf{Notation}	& \; \\
			\hline
			\textbf{Vir1\_32\_Def}                  & Virtuoso - single node - 32GB RAM - \\
			& Default Configuration\\
			\textbf{TPF3\_64\_Def}                  & Triple Pattern Fragments - 3 slave nodes - 64GB RAM - \\
			& Default Configuration\\
			\textbf{Gra1\_64\_Opt}                  & GraphDB - single node - 64GB RAM - \\
			& Optimized (RFI) Configuration\\
			\hline
		\end{tabular}
	}
	 \caption*{A description consists of a 3-character prefix describing the RDF storage solution, the number of nodes, the amount of memory and the configuration.}
\end{table}

%Benchmark Survival
Finally, a subtle error can be made in query runtime comparison for benchmarks which involve a query engine that becomes unresponsive (engine failure). In the runtime comparisons we only consider the range between the first query and the last successful query. We coin this the \emph{benchmark survival interval}, this is shown in Figure~\ref{fig:Fig03_BenchmarkSurvival_Other_Watdiv_Default}.

In Table~\ref{table:namingconv} we introduce a naming convention to describe the different benchmark setups. 

\subsection{Increasingly Large Datasets}
\label{subsec:bigdata}
\input{results_datasetsize.tex}

\subsection{Vertical Scaling}
\label{subsec:vscaling}
\input{results_verticalscaling.tex}

\subsection{RFI: Optimized Configurations}
\label{subsec:rfi}
\input{results_optimized.tex}

\subsection{Semantic Web Solutions}
\label{subsec:semweb}
\input{results_semweb.tex}

\subsection{Horizontal scaling}
\label{subsec:hscaling}
\input{results_horizontalscaling.tex}















