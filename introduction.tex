%put back Ruben's first version
%which ~ by the way moet erbij kunnen. which voor extra info, maar bij aanwijzing moet THAT

The vision of the \emph{Semantic Web} is to serve as a global distributed database which can be autonomously explored by intelligent agents. The Linked Open Data (LOD) project is an implementation of this vision and already contains over 1,000 interconnected datasets. 

Semantic Web technology has a lot to offer to multidisciplinary research domains.
For example, Life Sciences span multiple domains ranging from pharmacy to genetics to clinical trials. In the LOD cloud prominent Life Sciences datasets are 
UniProt (protein function), Drugbank (drugs) and chEMBL (properties of molecules).

%This necessitates the runtime integration of different datasets of significant size. 
Being able to interact with these datasets as one virtual source requires technology capable of both managing Big Linked Data, as well as successfully answering complex federated queries.


%\vfill %otherwise latex stress linespacing to fill page
%These challenges are being addressed by:
%\begin{itemize}
%	\item \emph{Vertical scaling} is using an expensive high-end machine with a lot of RAM and CPU power. 
%	No additional software development is required in this case.
%	\item A \emph{Compression algorithm} such as HDT~\cite{DBLP:journals/ws/FernandezMGPA13} can easily compress RDF datasets by a factor of 10--20. This allows for much larger datasets to be handled 
%	by a single server.
%\end{itemize}
%An alternative is to opt for a distributed architecture:
%\begin{itemize}
%	\item \emph{Horizontal scaling} uses multiple - often cheap, low-end - instances in a distributed system. 
%	Most enterprise RDF stores support parallelization, but this can imply both a high availability
%	solution (data replication), or a sharded system (data partitions) that can deal with increasingly large datasets. 
%	\item \emph{Query federation}~\cite{DBLP:conf/semweb/SchwarteHHSS11}: All datasets are hosted by their providers and
%	a federated query engine redirects the relevant parts of each query to the right endpoint and finally combines all the recieved information to solve the query.
%	\item Native \emph{Big Data approaches} typically map SPARQL queries to SQL technologies available in the Hadoop stack: SparkSQL~\cite{cure2015evaluation} or Impala~\cite{DBLP:conf/semweb/SchatzlePNL14}.
%\end{itemize}
%Citaat SWAT4LS:The Life Sciences domain is interdisciplinary, which makes interlinking data sources interesting and crucial.
%The Linked Open Data Cloud contains many RDF data sources related to life sciences, but this comes with a set of challenges: 
%(i) the union of all datasets qualifies as Big Data and therefore puts a strain on the available technologies for querying and 
%(ii) these insights contain information of multiple datasets at once, 
%making the queries federated in nature.
%challenges we address: more repeatable and easier to run your own benchmarks
\subsection{Challenges}
Choosing an RDF database and system architecture requires making trade-offs: 

\begin{itemize}
	\item What features are required for the system of choice given the use case at hand, which features are optional?
	\item What hardware is required to achieve a certain performance?
	\item What are the risks when using research prototypes instead of mature vendor-backed solutions? 
\end{itemize}

To make matters even more complicated, database vendors are continuously improving their products rendering previous benchmark results quickly outdated.
The goal of this work is to give an up-to-date view on the RDF storage solution space.
By providing scripts for deployment and post-processing of results, we provide a~feasible approach to run benchmarks with own data and queries using only a limited time window of a couple of days.
This work also offers a methodology to make benchmarks more reproducible and therefore the results more easily generally applicable.


\subsection{Research Questions}

The work presented here is built around 4 research questions:

\begin{description}
\item[RQ1] \emph{How to run a query performance benchmark in a reproducible and reliable way?}
%Methods hardware, configs, wrapper, hardware, amis,... query completeness diagnostics
\item[RQ2] \emph{What are the different options and the associated trade-offs when choosing a linked data infrastructure setup in the context of Big Linked Data? How can different setups be compared?}
%Options: Horizontal, vertical, compression, federation big data approaches, multithreading
\item[RQ3] \emph{What is the relative influence on the measured performance of contextual factors (for example: caching) for the different RDF solutions? Is the impact similar for all solutions?}
%query type, context=caching and server load, query completeness
\item[RQ4] \emph{How do the RDF systems behave in a real-world setting? Can we extract insights that might be transferred and generate unbiased insights and hypotheses to be verified in future benchmarks?}
%query types, real world vs Watdiv, Watdiv sizes, Decisiontree = unbiased
\end{description} 

\textbf{RQ1} will be mostly addressed in the section `Benchmark Approach'. Reliability is the focus of result sections `Query Result Completeness' and `Benchmark Error Analysis', 
%\ref{subsec:completeness} and \ref{subsec:erroranalysis} 
which deal with errors and query completeness. 

The impact of dataset size and the different approaches to scaling are discussed in the section `Results~I'.
%is analyzed in section \ref{subsec:bigdata},  \ref{subsec:vscaling} - %\ref{subsec:hscaling}. 
How all approaches can be compared, is the subject of section `Benchmark Cost', %\ref{sec:bmcost}, 
providing an answer to \textbf{RQ2}. 

In answering \textbf{RQ3}, we will show that focusing on `query runtime' alone is an oversimplification. We discuss multiple factors related to the query context in section `Results~II'. %\ref{sec:runtimefactors}. 
We also distinguish between \emph{average} and \emph{median} query runtimes, 
%in section \ref{sec:tradeoffs}, 
a distinction which can have a major impact on the perceived performance.

The final research question \textbf{RQ4} is dealt with in section `Results~III'
%\ref{sec:realworld}, 
where a custom Life Sciences benchmark is run in the context of federated faceted browsing. 
