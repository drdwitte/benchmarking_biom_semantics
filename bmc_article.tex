%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
%\documentclass{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\def\includegraphic{}
\def\includegraphics{}


%%% Put your definitions there:
\startlocaldefs
\endlocaldefs
%\usepackage[numbers]{natbib}

%\usepackage{amsmath}
\usepackage{dcolumn}
%\usepackage{endnotes}
%\usepackage{graphics}
\usepackage{graphicx}
%\usepackage[dvipsnames,svgnames]{xcolor}
\usepackage{enumerate}
\usepackage{float}
\usepackage{url}
\usepackage{csquotes}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{epstopdf}

%has \checkmark
\usepackage{amssymb}
\usepackage{multirow}

\newcolumntype{d}[1]{D{.}{.}{#1}}
\newcommand{\sql}[1]{\textsc{\scalebox{0.8}{#1}}}


\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}
%\usepackage{lmodern}% http://ctan.org/pkg/lm
%\usepackage[T1]{fontenc}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	filecolor=black,      
	urlcolor=black,
	citecolor=black
}
\urlstyle{same}

%added myself to deal with table captions + descriptions
%HOWTO: https://mirror.hmc.edu/ctan/macros/latex/contrib/caption/caption-eng.pdf
\usepackage{caption} 
\captionsetup[table]{skip=10pt, font={footnotesize}, labelfont={bf}}
	%textfont=footnotesize}
\captionsetup[figure]{skip=10pt, font={footnotesize}, labelfont={bf}}


\usepackage{array}% http://ctan.org/pkg/array
\renewcommand{\arraystretch}{1.5}%

\usepackage{titlesec}

%https://tex.stackexchange.com/questions/108684/spacing-before-and-after-section-titles
%\titlespacing*{<command>}{<left>}{<before-sep>}{<after-sep>}
\titlespacing*{\section}
{0pt}{\baselineskip}{\baselineskip}

\titlespacing*{\subsection}
{0pt}{\baselineskip}{\baselineskip}

\usepackage{enumitem}
\setlist{nosep,after=\vspace{\baselineskip}, before=\vspace{\baselineskip}}


\renewcommand\labelitemi{-}

%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Reproducible Query Performance Assessment of Scalable RDF Storage Solutions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={A},                   % id's of addresses, e.g. {aff1,aff2}
%   corref={A},                       % id of corresponding address, if any
%   noteref={n1},                        % id's of article notes, if any
   email={drdwitte@gmail.com}   % email address
]{\inits{DDW}\fnm{Dieter} \snm{De Witte}}
\author[
addressref={A},                   % id's of addresses, e.g. {aff1,aff2}
]{\inits{LDV}\fnm{Laurens} \snm{De Vocht}}
\author[
addressref={A},
]{\inits{DDP}\fnm{Dieter} \snm{De Paepe}}
\author[
addressref={B},
]{\inits{FP}\fnm{Filip} \snm{Pattyn}}
\author[
addressref={B},
]{\inits{KK}\fnm{Kenny} \snm{Knecht}}
\author[
addressref={B},
]{\inits{HC}\fnm{Hans} \snm{Constandt}}
\author[
addressref={A},
]{\inits{JF}\fnm{Jan} \snm{Fostier}}
\author[
addressref={A},
]{\inits{RV}\fnm{Ruben} \snm{Verborgh}}
\author[
addressref={A},
]{\inits{EM}\fnm{Erik} \snm{Mannens}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=A]{%                           % unique id
  \orgname{imec - IDLab - Ghent University}, % university, etc
  \street{iGent Tower - Technologiepark-Zwijnaarde 15},                     %
  \postcode{BE 9052}                                % post or zip code
  \city{Ghent},                              % city
  \cny{Belgium}                                    % country
}
\address[id=B]{%
  \orgname{Ontoforce},
  \street{Technologiepark-Zwijnaarde 19},                     %
  \postcode{BE 9052}                                % post or zip code
  \city{Ghent},                              % city
  \cny{Belgium}                                    % country
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}

%\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract}
\parttitle{Background} 
%CONTEXT
Applications in the biomedical domain rely on Linked Data spanning multiple datasets for an increasing number of use cases. %\todo{DDP: only biomedical? all biomed apps?}
Choosing a strategy for running federated queries over Big Linked Data is however a challenging task.
%Biomedisch data, federated, big data, schaal, artificiele

%NEED
\noindent Given the abundance of Linked Data storage solutions and benchmarks,
it is not straightforward to make an informed choice between platforms.
%Hoe objectief kiezen? Snel kiezen

%TASK
\noindent This can be addressed by releasing an updated review of the state-of-the-art periodically and by providing tools and methods to make these more (easily) reproducible. 
Running a custom benchmark tailored to a specific use case becomes more feasible by simplifying deployment, configuration, and post-processing.
%Automatiseren van deployment en postprocessing, objectief maken = repeatable
\parttitle{Results}
%OBJECT
The results in this work are obtained by performing an extensive query performance benchmark. The focus lies on comparing scalable RDF systems and iterating over different hardware options and engine configurations. Contrary to most benchmarking efforts, comparisons are made across different approaches to Linked Data querying by comparing the financial benchmark costs. Both artificial tests and a real case with queries from a biomedical search application are analyzed. To make the interpretation of the benchmark results more reproducible we relied decision trees trained on query features.

%FINDINGS
\noindent In analyzing the performance results, we discovered that single-node triple stores benefit greatly from vertical scaling and proper configuration. Results show that horizontal scalability is still a real challenge to most systems. 
Semantic Web storage solutions based on federation, compression, or Linked Data Fragments still lag by an order of magnitude in terms of performance.  
Furthermore, we demonstrate the need for careful analysis of contextual factors influencing query runtimes: server load, availability, caching effects, and query completeness all perturb the benchmark results.
%Horizontale schaalbaarheid, TPF, Query correctness
%CONCLUSION
\parttitle{Conclusions}
With this work we offer a reusable methodology to facilitate comparison between existing and future query performance benchmarks. We release our results in a rich event format ensuring reproducibility while also leaving room for serendipity. 
%Objectiviteit belangrijk, TPF
%OUTLOOK	
%Hopelijk wordt methodologie gevolgd. Future benchmarks fixed methodology
This methodology facilitates the integration with future benchmark results.
	
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{Benchmarks}
\kwd{Benchmarking Tools}
\kwd{Big Linked Data}
\kwd{Distributed Querying}
\kwd{Life Sciences}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>



\section{Introduction}
\input{introduction.tex}

%\subsection{Research collaboration with Ontoforce}
%\input{ontoforce.tex}

\subsection{Our Contribution}
\input{ourcontribution.tex}

\section{Related Work}
\input{relatedwork.tex}
%Results S2RDF vs Virtuoso32GB an order of magnitude faster but 10 nodes versus 1.

%~\footnote{\scriptsize \url{https://bitbucket.org/openrdf/alibaba/}}
%http://www.mulgara.org/
%https://neo4j.com/
%https://www.postgresql.org/

\section{Benchmark approach}
\input{methods.tex}

\subsection{Store Preselection: Feature matrix}
\label{subsec:featurematrix}
\input{featurematrix.tex}

\subsection{A quick and reusable benchmarking scheme}
\label{subsec:bmscheme}
\input{reusable.tex}

\subsection{Datasets and Queries}
\label{subsec:dataqueries}
\input{dataset.tex}

%\subsection{Benchmark Cost}
%\input{benchmarkcost.tex}

\section{Results I: Approaches to Linked Data at Scale}
\label{sec:tradeoffs}
\input{resultsI.tex}

\section{Results II: Query Runtime Analysis in Depth}
\label{sec:runtimefactors}
\input{resultsII.tex}

\section{Results III: Real-world Life Sciences Benchmark Results}
\label{sec:realworld}
\input{resultsIII.tex}

\section{Conclusion}
\input{conclusion.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Ethics approval and consent to participate}
Not applicable

\section*{Consent for publication}
Not applicable

\section*{Availability of data and materials}

\todo{onderstaande wat tunen}
All data generated or analysed during this study are included in this published article [and its supplementary information files].

The data that support the findings of this study are available from Ontoforce but restrictions apply to the availability of these data, which were used under license for the current study, and so are not publicly available. Data are however available from the authors upon reasonable request and with permission of [third party name].

\section*{Competing interests}
Some of the co-authors in this research paper have contributed to the TPF implementation. (LDV and RV) Te other authors have no competing interests.

\section*{Funding}
The research activities were funded by VLAIO (the Agency for Innovation and
Entrepreneurship in Flanders) in an R\&D project called SEQUEL with Ontoforce, Ghent University and imec (iMinds). 

\section*{Author's contributions}
The work presented in this paper was conducted in collaboration between all authors. DDW and
LDV conducted the experiments. DDW analyzed the data and drafted the paper. FP, KK and HC helped defining a relevant Life Sciences use case and provided datasets and queries. JF, RV and EM revised the research paper and coordinated the research process. All authors have approved the final version of the manuscript. 

\section*{Acknowledgements}
We would like to acknowledge iLab.t who provided the high memory infrastructure to compress the benchmark datasets and a set of servers to run additional benchmarks.\\
We would also like to express our gratitude for the support provided by the staff working for the 4 vendors (Blazegraph, GraphDB, Virtuoso, FluidOps) we were allowed to disclose and to Rob Vesse who provided support for the SPARQL Query Benchmarker software.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}

%\begin{figure}[ht!]
%  \caption{\csentence{Sample figure title.}
%      A short description of the figure content
%      should go here.}
%      \end{figure}
 
\todo{Replace Flu with FedX inside figures!}
  
%1    
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.9\linewidth]{imgs/Fig01_WatdivNoSQLDataScaling}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{ \csentence{Query runtime distributions of \emph{Vendor} systems for 3 different sizes of WatDiv.} Dots correspond to average runtimes, while the horizontal lines in the box plots correspond to median runtimes. The difference is scaling behavior between \textbf{Vir\_32} (linear) and the other stores emphasizes the different impact of server memory on runtime behavior. \textbf{Bla1\_32} and \textbf{Gra1\_32} are very close in terms of average runtimes, for individual queries GraphDB is superior except when scaling up to WatDiv1000M. \textbf{ES1\_32} is the only store with timeout problems starting from WatDiv100M. }
	\label{fig:Fig01_WatdivNoSQLDataScaling}
\end{figure} 
   
%2     
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.9\linewidth]{imgs/Fig02_WatdivVerticalScaling}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Query runtime distributions for WatDiv1000M showing the effect of increasing memory from 32GB (left) to 64GB (center) and \emph{Optimized} configurations (right)}. Virtuoso hardly doesn't benefit from additional memory or better configurations. GraphDB is the most sensitive to proper configuration. In the right panel engine performance starts converging. In terms of average runtimes \textbf{Bla1\_64\_Opt} is the fastest, in terms of median runtimes both \textbf{Vir1\_64\_*} setups perform best.}
	\label{fig:Fig02_WatdivVerticalScaling}
\end{figure}

%explained in text why there is only a single FedX1 simulation => queries are the same!
%3      
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.99\linewidth]{imgs/Fig03_BenchmarkSurvival_Other_Watdiv_Default}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Benchmark survival interval for 3 \emph{Prototypes}.} For early crashes the amount of queries until system failure is reported, as well as the query template causing the failure. \textbf{FedX3\_64} crashes upon the first occurrence of a \textbf{C3} query. \textbf{Fus1\_64} survives the warm-up run for WatDiv100M but crashes upon the first occurrence of a \textbf{C2} query in the stress test, for WatDiv1000M again the first \textbf{C2} query in the warm-up run causes the crash.}
	\label{fig:Fig03_BenchmarkSurvival_Other_Watdiv_Default}
\end{figure}   

%4
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.9\linewidth]{imgs/Fig04_WatdivHorizontalScaling}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Pairwise comparison of query runtime distributions for single-node versus 3-node setups}. None of the solutions achieve an average runtime speedup when adding more nodes, on the contrary overhead multipication factors of 1.9 and 1.5 are seen in left and center pane for \textbf{Vir3\_32\_Def} and \textbf{ES3\_32\_Def}. For \textbf{TPF3\_64\_Def} the overhead is negligible.}
	\label{fig:Fig04_WatdivHorizontalScaling}
\end{figure}

%5
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.9\linewidth]{imgs/Fig05_WatdivTemplates}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Average Runtime per query template for 5 single-node setups.} \textbf{TPF1\_64} has only 5 templates which do not coincide with the timeout of 300s, for \textbf{ES1\_64\_Def} this is alread 15 templates.
		\textbf{Vir1\_64\_Opt} is the fastest engine for 13 templates, \textbf{Gra1\_64\_Opt} for and \textbf{Bla1\_64\_Opt} for 3 templates each. Template \textbf{C3} was omitted due to query completeness issues. Blazegraph was the only engine to retrieve all results. } 
	\label{fig:Fig05_WatdivTemplates}
\end{figure}
 
%6
\begin{figure}[ht!]
	\centering
 	%\includegraphics[width=0.9\linewidth]{imgs/Fig06_WatdivTemplateTypes}
 	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
 	\caption{\csentence{Average Runtime per BGP type.}}
 	\label{fig:Fig06_WatdivTemplateTypes}
\end{figure}
   
%7   
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.90\linewidth]{imgs/Fig07_Watdiv_SingleMultiClient}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Runtimes for single versus multi-client workloads: 1 vs. 5 threads.} 
		5T runtime corresponds to the maximum runtime per query in the stress test, 1T is the runtime during the warm-up phase. The red line corresponds to the bisector, where the runtime for both workloads is equal. Dots are expected to be shifted up, which correspond to a multiplication factor. The closer the dots to the bisector the smaller the multi-client overhead. Dots below the bisector can be attributed to the natural variance in query runtimes. Average runtimes per store are also shown. \textbf{Bla1\_64} and \textbf{Vir1\_64} have the smallest overhead $(< 20\%)$, for \textbf{ES1\_64} has the largest $(> 300\%)$.
	}
	\label{fig:Fig07_Watdiv_SingleMultiClient}
\end{figure}

%8
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.9\linewidth]{imgs/Fig08_Watdiv_caching}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Speedup in query runtime.} We compare query runtimes in the multi-threaded run with the slowest execution in the stress test. With no caching all dots are expected on the X and Y-axis, the latter because of the noise on small query runtimes. If we focus on speedups $> 2$, especially \textbf{ES1} and \textbf{TPF*} seem to have the highest benefit.  }
	\label{fig:Fig08_Watdiv_caching}
\end{figure}

%9
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.90\linewidth]{imgs/Fig09_FailuresOntoforceBM}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Overview of successes and errors per query (Y-axis) and thread (X-axis) on the Ontoforce benchmark.}
		Queries are sorted per system in order to group error behavior and are not consistent between simulations!
		Blazegraph has a short benchmark survival interval. \textbf{ES1}, \textbf{Gra1} and \textbf{Vir3} Cluster setups have a lot of errors but most queries execute successfully at least once, which allows runtime comparisons. \textbf{Vir3\_64} was re-run multiple times and labeled with an additional index: \textbf{Vir3\_64\_Opt\_0} is the most successful Virtuoso cluster run as query completeness analysis revealed that \textbf{Vir3\_64\_Opt\_2} has unreported errors for 37\% of the queries.
	}
	\label{fig:Fig09_FailuresOntoforceBM}
\end{figure}   

%10
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.75\linewidth]{imgs/Fig10_AllTrees}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Decision Tree Analysis to identify the reason for query failure, certain error types, and high/low query runtimes.} Input for all trees are feature vectors, also the query engine is added as a categorical feature. Rules in the decision trees are shown in red, sample sizes are encoded as the width of the bottom bar and the value is added inside the bars in bold. For each separate part the class distribution or the average runtime is reported below the bar. 
		\newline \hspace{\linewidth} 	
		\underline{\smash{Top:}} Classification into query success (blue) and failure (red) and incomplete. The query engine is an important decision rule, which demonstrates that Virtuoso behaves very different from the other systems.  
		\newline \hspace{\linewidth} 	
		\underline{\smash{Center:}} Classification of query failures into classes incomplete (orange), server error (green), and timeout (purple). 
		\newline \hspace{\linewidth} 	
		\underline{\smash{Bottom:}}  Regression on query runtimes. Red corresponds to high query runtimes, white to low. }
	\label{fig:Fig10_AllTrees}
\end{figure}

%11
\begin{figure}[ht!]
	\centering
	%\includegraphics[width=0.9\linewidth]{imgs/Fig11_AllSims_Correct}
	\captionsetup{skip=10pt, font={footnotesize}, labelfont={bf}, width=0.4\textwidth, margin=1cm}
	\caption{\csentence{Benchmark Cost in \$ to load and execute 2000 queries in a stress test for WatDiv1000M or Ontoforce datasets for different setups.} All stacked bars consists the load cost stacked on top of the runtime cost. Bar width encodes the amount of nodes. For WatDiv \textbf{Vir1\_32\_Def} is the least expensive solution, mainly because \textbf{Bla1\_64\_Opt} has a much higher load cost. Also for the Ontoforce benchmark\textbf{ Vir1\_32\_Opt} is the most cost-effective choice. The engine ranking is not conserved going from artificial to real-world benchmark.}
	\label{fig:Fig11_AllSims_Correct}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
%\section*{Tables}
%\begin{table}[h!]
%\caption{Sample table title. This is where the description of the table should go.}
%      \begin{tabular}{cccc}
%        \hline
%           & B1  &B2   & B3\\ \hline
%        A1 & 0.1 & 0.2 & 0.3\\
%        A2 & ... & ..  & .\\
%        A3 & ..  & .   & .\\ \hline
%      \end{tabular}
%\caption*{Text below}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}
  \subsection*{Sequel Project Website}
    The website links to all material related to this manuscript: datasets, notebooks with analysis, raw log files,\ldots can be found here: \url{http://users.elis.ugent.be/~drdwitte/index.html}

  \subsection*{Feature Matrix}
  Overview om Semantic Databases considered in this benchmark with together with a set of
  features and links where this information was found:   \url{http://users.elis.ugent.be/~drdwitte/featurematrix.html}
  \subsection*{Notebooks and CSV files for postprocessing}
  All CSV files with different views on the benchmark output together with the Jupyter notebook files showing the original analysis of the data can be found here: \url{http://users.elis.ugent.be/~drdwitte/postprocessing.html}
\end{backmatter}
\end{document}
