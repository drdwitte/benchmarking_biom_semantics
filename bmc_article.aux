\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\global\@namedef{num@address}{2}
\global\@namedef{num@author}{9}
\global\authorcorref@notsettrue
\gdef\hy@title{Reproducible Query Performance Assessment of Scalable RDF Storage Solutions}
\thanksnewlabel{au1@email}{{drdwitte@gmail.com}{1}}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\gdef\hy@fauthor{Dieter De Witte}
\gdef\hy@author{Dieter De Witte, Laurens De Vocht, Dieter De Paepe, Filip Pattyn, Kenny Knecht, Hans Constandt, Jan Fostier, Ruben Verborgh, Erik Mannens}
\gdef\hy@subject{}
\gdef\hy@keywords{Benchmarks, Benchmarking Tools, Big Linked Data, Distributed Querying, Life Sciences}
\@writefile{toc}{\contentsline {section}{\numberline {1}TODOfigures}{1}{section.1}}
\thanksnewlabel{au1thanks}{{*}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{1}{section.2}}
\citation{de2016big}
\citation{alucc2014diversified}
\citation{dewitte_swat4ls_2016}
\citation{DBLP:journals/corr/abs-1103-5043}
\citation{DBLP:conf/semweb/VerborghHMHVSCCMW14}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Challenges}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Prior Results}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Research Questions}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Our Contribution}{2}{subsection.2.4}}
\citation{graux2016multi}
\citation{ladwig2011cumulusrdf}
\citation{Schatzle:2016:SRQ:2977797.2977806}
\citation{schatzle2011pigsparql}
\citation{kotsevbenchmarking}
\citation{hernandez2016querying}
\citation{harris20094store}
\citation{Schatzle:2016:SRQ:2977797.2977806}
\citation{Schatzle:2016:SRQ:2977797.2977806}
\citation{papailiou2013h}
\citation{schatzle2014sempala}
\citation{schatzle2011pigsparql}
\citation{rohloff2010high}
\citation{Saleem}
\citation{DBLP:conf/semweb/SchwarteHHSS11}
\citation{gorlitz2011splendid}
\citation{Acosta2011}
\citation{saleem2014hibiscus}
\citation{saleem2015feasible}
\citation{alucc2014diversified}
\citation{boncz2005monetdb}
\citation{neumann2010rdf}
\citation{zou2011gstore}
\citation{cudre2013nosql}
\citation{khadilkar2012jena}
\citation{wu2014biobenchmark}
\citation{Schmidt2011}
\citation{guo2005lubm}
\citation{schmidt2009sp}
\citation{bizer2009berlin}
\citation{morsey2011dbpedia}
\citation{alucc2014diversified}
\citation{saleem2015feasible}
\citation{kotsevbenchmarking}
\citation{DBLP:conf/sigmod/DuanKSU11}
\citation{wu2014biobenchmark}
\citation{antezana2009cell}
\citation{yamamoto2011allie}
\citation{kinjo2011protein}
\citation{uniprot2014uniprot}
\citation{tateno2002dna}
\citation{Schmidt2011}
\citation{Saleem}
\citation{gorlitz2012splodge}
\citation{hernandez2016querying}
\citation{graux2016multi}
\citation{ladwig2011cumulusrdf}
\citation{ghemawat2003google}
\citation{Schatzle:2016:SRQ:2977797.2977806}
\citation{schatzle2011pigsparql}
\citation{cudre2013nosql}
\citation{Schatzle:2016:SRQ:2977797.2977806}
\citation{LDBC}
\citation{HOBBIT}
\citation{Boncz:2013:LBG:2513591.2527070}
\citation{erling2015ldbc}
\citation{kotsevbenchmarking}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{3}{section.3}}
\citation{Picalausa2011}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Overview of recent (2011-2016) benchmarking results.}}{4}{table.1}}
\newlabel{benchmarks}{{1}{4}{Overview of recent (2011-2016) benchmarking results}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Positioning this work}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Up-to-date view}{4}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Scalability}{4}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Broad set of query types}{4}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Query Correctness}{4}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Objective and exhaustive}{4}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Multi-setup}{4}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Query-mix size}{4}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Flexibility}{4}{section*.9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Benchmark approach}{4}{section.4}}
\citation{Stegmaier_evaluationof}
\citation{DBLP:journals/ws/FernandezMGPA13}
\citation{DBLP:conf/semweb/SchwarteHHSS11}
\citation{DBLP:conf/semweb/VerborghHMHVSCCMW14}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Benchmark Space Exploration}{5}{subsection.4.1}}
\newlabel{subsec:bmexplore}{{4.1}{5}{Benchmark Space Exploration}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Overview of benchmarks run in this study.}}{5}{table.2}}
\newlabel{bmspace}{{2}{5}{Overview of benchmarks run in this study}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Store Preselection: Feature matrix}{5}{subsection.4.2}}
\newlabel{subsec:featurematrix}{{4.2}{5}{Store Preselection: Feature matrix}{subsection.4.2}{}}
\citation{DBLP:conf/semweb/SchwarteHHSS11}
\citation{saleem2016fine}
\citation{de2016big}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces List of the tested systems and their acronyms. The first four are enterprise systems. }}{6}{table.3}}
\newlabel{acronyms}{{3}{6}{List of the tested systems and their acronyms. The first four are enterprise systems}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}A quick and reusable benchmarking scheme}{6}{subsection.4.3}}
\newlabel{subsec:bmscheme}{{4.3}{6}{A quick and reusable benchmarking scheme}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Reproducible hardware}{6}{subsubsection.4.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Instance types used in benchmarks and their purpose.}}{6}{table.4}}
\newlabel{instances}{{4}{6}{Instance types used in benchmarks and their purpose}{table.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Reproducible installations and configurations}{6}{subsubsection.4.3.2}}
\citation{de2016big}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Different configuration choices defined in this benchmark}}{7}{table.5}}
\newlabel{table:configs}{{5}{7}{Different configuration choices defined in this benchmark}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Reusable Benchmark Components}{7}{subsubsection.4.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Content of the query events used as the starting point for all benchmark results in this work}}{7}{table.6}}
\newlabel{table:queryevents}{{6}{7}{Content of the query events used as the starting point for all benchmark results in this work}{table.6}{}}
\citation{alucc2014diversified}
\citation{Zeng}
\citation{Harth}
\citation{dewitte_swat4ls_2016}
\citation{Ferre}
\citation{Oren}
\citation{Picalausa2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Datasets and Queries}{8}{subsection.4.4}}
\newlabel{subsec:dataqueries}{{4.4}{8}{Datasets and Queries}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Datasets.}{8}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Queries.}{8}{section*.11}}
\citation{Groth}
\citation{de2016big}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results I: Approaches to Linked Data at Scale}{9}{section.5}}
\newlabel{sec:tradeoffs}{{5}{9}{Results I: Approaches to Linked Data at Scale}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Conventions for describing benchmark setups. A description consists of a 3-character prefix describing the RDF storage solution, the number of nodes, the amount of memory and the configuration.}}{9}{table.7}}
\newlabel{table:namingconv}{{7}{9}{Conventions for describing benchmark setups. A description consists of a 3-character prefix describing the RDF storage solution, the number of nodes, the amount of memory and the configuration}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Increasingly Large Datasets}{9}{subsection.5.1}}
\newlabel{subsec:bigdata}{{5.1}{9}{Increasingly Large Datasets}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Query runtime distributions of \emph  {Vendor} systems for 3 different sizes of WatDiv. Dots correspond to average runtimes, while the horizontal lines in the box plots correspond to median runtimes. The difference is scaling behavior between \fig@textbf  {Vir\_32} (linear) and the other stores emphasizes the different impact of server memory on runtime behavior. \fig@textbf  {Bla1\_32} and \fig@textbf  {Gra1\_32} are very close for batch workloads, for individual queries GraphDB is superior except when scaling up to WatDiv1000M. \fig@textbf  {ES1\_32} is the only store with timeout problems starting from WatDiv100M. }}{10}{figure.1}}
\newlabel{fig:Fig01_WatdivNoSQLDataScaling}{{1}{10}{Query runtime distributions of \emph {Vendor} systems for 3 different sizes of WatDiv. Dots correspond to average runtimes, while the horizontal lines in the box plots correspond to median runtimes. The difference is scaling behavior between \textbf {Vir\_32} (linear) and the other stores emphasizes the different impact of server memory on runtime behavior. \textbf {Bla1\_32} and \textbf {Gra1\_32} are very close for batch workloads, for individual queries GraphDB is superior except when scaling up to WatDiv1000M. \textbf {ES1\_32} is the only store with timeout problems starting from WatDiv100M}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Vertical Scaling}{10}{subsection.5.2}}
\newlabel{subsec:vscaling}{{5.2}{10}{Vertical Scaling}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}RFI: Optimized Configurations}{10}{subsection.5.3}}
\newlabel{subsec:rfi}{{5.3}{10}{RFI: Optimized Configurations}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Semantic Web Solutions}{10}{subsection.5.4}}
\newlabel{subsec:semweb}{{5.4}{10}{Semantic Web Solutions}{subsection.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Query runtime distributions for WatDiv1000M showing the effect of increasing memory from 32GB (left) to 64GB (center) and \emph  {Optimized} configurations (right). Virtuoso hardly doesn't benefit from additional memory or better configurations. GraphDB is the most sensitive to proper configuration. In the right panel engine performance starts converging. For batch workloads \fig@textbf  {Bla1\_64\_Opt} is the fastest, in terms of median runtimes both \fig@textbf  {Vir1\_64\_*} setups perform best.}}{11}{figure.2}}
\newlabel{fig:Fig02_WatdivVerticalScaling}{{2}{11}{Query runtime distributions for WatDiv1000M showing the effect of increasing memory from 32GB (left) to 64GB (center) and \emph {Optimized} configurations (right). Virtuoso hardly doesn't benefit from additional memory or better configurations. GraphDB is the most sensitive to proper configuration. In the right panel engine performance starts converging. For batch workloads \textbf {Bla1\_64\_Opt} is the fastest, in terms of median runtimes both \textbf {Vir1\_64\_*} setups perform best}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Benchmark survival interval for 3 \emph  {SemWeb} solutions. For early crashes the amount of queries until system failure is reported, as well as the query template causing the failure. \fig@textbf  {Flu3\_64} crashes upon the first occurrence of a \fig@textbf  {C3} query. \fig@textbf  {Fus1\_64} survives the warm-up run for WatDiv100M but crashes upon the first occurrence of a \fig@textbf  {C2} query in the stress test, for WatDiv1000M again the first \fig@textbf  {C2} query in the warm-up run causes the crash.}}{11}{figure.3}}
\newlabel{fig:Fig03_BenchmarkSurvival_Other_Watdiv_Default}{{3}{11}{Benchmark survival interval for 3 \emph {SemWeb} solutions. For early crashes the amount of queries until system failure is reported, as well as the query template causing the failure. \textbf {Flu3\_64} crashes upon the first occurrence of a \textbf {C3} query. \textbf {Fus1\_64} survives the warm-up run for WatDiv100M but crashes upon the first occurrence of a \textbf {C2} query in the stress test, for WatDiv1000M again the first \textbf {C2} query in the warm-up run causes the crash}{figure.3}{}}
\citation{cure2015evaluation}
\citation{graux2016multi}
\citation{Schatzle:2016:SRQ:2977797.2977806}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Horizontal scaling}{12}{subsection.5.5}}
\newlabel{subsec:hscaling}{{5.5}{12}{Horizontal scaling}{subsection.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Pairwise comparison of query runtime distributions for single-node versus 3-node setups. None of the solutions achieve an average runtime speedup when adding more nodes, on the contrary overhead multipication factors of 1.9 and 1.5 are seen in left and center pane for \fig@textbf  {Vir3\_32\_Def} and \fig@textbf  {ES3\_32\_Def}. For \fig@textbf  {TPF3\_64\_Def} the overhead is negligible.}}{12}{figure.4}}
\newlabel{fig:Fig04_WatdivHorizontalScaling}{{4}{12}{Pairwise comparison of query runtime distributions for single-node versus 3-node setups. None of the solutions achieve an average runtime speedup when adding more nodes, on the contrary overhead multipication factors of 1.9 and 1.5 are seen in left and center pane for \textbf {Vir3\_32\_Def} and \textbf {ES3\_32\_Def}. For \textbf {TPF3\_64\_Def} the overhead is negligible}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average Runtime per query template for 5 single-node setups. \fig@textbf  {TPF1\_64} has only 5 templates which do not coincide with the timeout of 300s, for \fig@textbf  {ES1\_64\_Def} this is alread 15 templates. \fig@textbf  {Vir1\_64\_Opt} is the fastest engine for 13 templates, \fig@textbf  {Gra1\_64\_Opt} for and \fig@textbf  {Bla1\_64\_Opt} for 3 templates each. Template \fig@textbf  {C3} was omitted due to query completeness issues. Blazegraph was the only engine to retrieve all results. }}{13}{figure.5}}
\newlabel{fig:Fig05_WatdivTemplates}{{5}{13}{Average Runtime per query template for 5 single-node setups. \textbf {TPF1\_64} has only 5 templates which do not coincide with the timeout of 300s, for \textbf {ES1\_64\_Def} this is alread 15 templates. \textbf {Vir1\_64\_Opt} is the fastest engine for 13 templates, \textbf {Gra1\_64\_Opt} for and \textbf {Bla1\_64\_Opt} for 3 templates each. Template \textbf {C3} was omitted due to query completeness issues. Blazegraph was the only engine to retrieve all results}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results II: Query Runtime Analysis in Depth}{13}{section.6}}
\newlabel{sec:runtimefactors}{{6}{13}{Results II: Query Runtime Analysis in Depth}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Query runtimes for different template types}{13}{subsection.6.1}}
\newlabel{subsec:templates}{{6.1}{13}{Query runtimes for different template types}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Single versus Multi-client stress testing}{13}{subsection.6.2}}
\newlabel{subsec:load}{{6.2}{13}{Single versus Multi-client stress testing}{subsection.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average Runtime per BGP type.}}{14}{figure.6}}
\newlabel{fig:Fig06_WatdivTemplateTypes}{{6}{14}{Average Runtime per BGP type}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Runtimes for single versus multi-client workloads: 1 vs. 5 threads. 5T runtime corresponds to the maximum runtime per query in the stress test, 1T is the runtime during the warm-up phase. The red line corresponds to the bisector, where the runtime for both workloads is equal. Dots are expected to be shifted up, which correspond to a multiplication factor. The closer the dots to the bisector the smaller the multi-client overhead. Dots below the bisector can be attributed to the natural variance in query runtimes. Average runtimes per store are also shown. \fig@textbf  {Bla1\_64} and \fig@textbf  {Vir1\_64} have the smallest overhead $(< 20\%)$, for \fig@textbf  {ES1\_64} has the largest $(> 300\%)$. }}{14}{figure.7}}
\newlabel{fig:Fig07_Watdiv_SingleMultiClient}{{7}{14}{Runtimes for single versus multi-client workloads: 1 vs. 5 threads. 5T runtime corresponds to the maximum runtime per query in the stress test, 1T is the runtime during the warm-up phase. The red line corresponds to the bisector, where the runtime for both workloads is equal. Dots are expected to be shifted up, which correspond to a multiplication factor. The closer the dots to the bisector the smaller the multi-client overhead. Dots below the bisector can be attributed to the natural variance in query runtimes. Average runtimes per store are also shown. \textbf {Bla1\_64} and \textbf {Vir1\_64} have the smallest overhead $(< 20\%)$, for \textbf {ES1\_64} has the largest $(> 300\%)$}{figure.7}{}}
\citation{dewitte_swat4ls_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}The Role of Caching in Query Runtime Results}{15}{subsection.6.3}}
\newlabel{subsec:caching}{{6.3}{15}{The Role of Caching in Query Runtime Results}{subsection.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Query Result Completeness}{15}{subsection.6.4}}
\newlabel{subsec:completeness}{{6.4}{15}{Query Result Completeness}{subsection.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Results III: Real-world Life Sciences Benchmark Results}{15}{section.7}}
\newlabel{sec:realworld}{{7}{15}{Results III: Real-world Life Sciences Benchmark Results}{section.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Speedup in query runtime by comparing all query runtimes in the multi-threaded run with the slowest execution in the stress test. With no caching all dots are expected on the X and Y-axis, the latter because of the noise on small query runtimes. If we focus on speedups $> 2$, especially \fig@textbf  {ES1} and \fig@textbf  {TPF*} seem to have the highest benefit. }}{16}{figure.8}}
\newlabel{fig:Fig08_Watdiv_caching}{{8}{16}{Speedup in query runtime by comparing all query runtimes in the multi-threaded run with the slowest execution in the stress test. With no caching all dots are expected on the X and Y-axis, the latter because of the noise on small query runtimes. If we focus on speedups $> 2$, especially \textbf {ES1} and \textbf {TPF*} seem to have the highest benefit}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Overview of successes and errors per query (Y-axis) and thread (X-axis) on the Ontoforce benchmark. Queries are sorted per system in order to group error behavior and are not consistent between simulations! Blazegraph has a short benchmark survival interval. \fig@textbf  {ES1}, \fig@textbf  {Gra1} and \fig@textbf  {Vir3} Cluster setups have a lot of errors but most queries execute successfully at least once, which allows runtime comparisons. \fig@textbf  {Vir3\_64\_Opt\_0} is the most successful Virtuoso cluster run as query completeness analysis revealed that \fig@textbf  {Vir3\_64\_Opt\_2} has unreported errors for 37\% of the queries. }}{16}{figure.9}}
\newlabel{fig:Fig09_FailuresOntoforceBM}{{9}{16}{Overview of successes and errors per query (Y-axis) and thread (X-axis) on the Ontoforce benchmark. Queries are sorted per system in order to group error behavior and are not consistent between simulations! Blazegraph has a short benchmark survival interval. \textbf {ES1}, \textbf {Gra1} and \textbf {Vir3} Cluster setups have a lot of errors but most queries execute successfully at least once, which allows runtime comparisons. \textbf {Vir3\_64\_Opt\_0} is the most successful Virtuoso cluster run as query completeness analysis revealed that \textbf {Vir3\_64\_Opt\_2} has unreported errors for 37\% of the queries}{figure.9}{}}
\citation{dewitte_swat4ls_2016}
\citation{dewitte_swat4ls_2016}
\citation{dewitte_swat4ls_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Benchmark Error Analysis}{17}{subsection.7.1}}
\newlabel{subsec:erroranalysis}{{7.1}{17}{Benchmark Error Analysis}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Error Frequencies}{17}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Query Correctness.}{17}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Decision Tree Analysis of Query Features}{18}{subsection.7.2}}
\newlabel{subsec:dtrees}{{7.2}{18}{Decision Tree Analysis of Query Features}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Benchmark Cost}{18}{subsection.7.3}}
\newlabel{sec:bmcost}{{7.3}{18}{Benchmark Cost}{subsection.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Query Features and information on their range and correlations with other (discarded) features.}}{19}{table.8}}
\newlabel{table:features}{{8}{19}{Query Features and information on their range and correlations with other (discarded) features}{table.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Decision Tree Analysis to identify the reason for query failure, certain error types, and high/low query runtimes. Input for all trees are feature vectors, also the query engine is added as a categorical feature. Rules in the decision trees are shown in red, sample sizes are encoded as the width of the bottom bar and the value is added inside the bars in bold. For each separate part the class distribution or the average runtime is reported below the bar. \newline  \hspace  {\linewidth } \relax $\@@underline {\hbox {\smash {Top:}}}\mathsurround \z@ $\relax  Classification into query success (blue) and failure (red) and incomplete. The query engine is an important decision rule, which demonstrates that Virtuoso behaves very different from the other systems. \newline  \hspace  {\linewidth } \relax $\@@underline {\hbox {\smash {Center:}}}\mathsurround \z@ $\relax  Classification of query failures into classes incomplete (orange), server error (green), and timeout (purple). \newline  \hspace  {\linewidth } \relax $\@@underline {\hbox {\smash {Bottom:}}}\mathsurround \z@ $\relax  Regression on query runtimes. Red corresponds to high query runtimes, white to low. }}{19}{figure.10}}
\newlabel{fig:Fig10_AllTrees}{{10}{19}{Decision Tree Analysis to identify the reason for query failure, certain error types, and high/low query runtimes. Input for all trees are feature vectors, also the query engine is added as a categorical feature. Rules in the decision trees are shown in red, sample sizes are encoded as the width of the bottom bar and the value is added inside the bars in bold. For each separate part the class distribution or the average runtime is reported below the bar. \newline \hspace {\linewidth } \underline {\smash {Top:}} Classification into query success (blue) and failure (red) and incomplete. The query engine is an important decision rule, which demonstrates that Virtuoso behaves very different from the other systems. \newline \hspace {\linewidth } \underline {\smash {Center:}} Classification of query failures into classes incomplete (orange), server error (green), and timeout (purple). \newline \hspace {\linewidth } \underline {\smash {Bottom:}} Regression on query runtimes. Red corresponds to high query runtimes, white to low}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Benchmark Cost in \$ to load and execute 2000 queries in a stress test for WatDiv1000M or Ontoforce datasets for different setups. All stacked bars consists the load cost stacked on top of the runtime cost. Bar width encodes the amount of nodes. For WatDiv \fig@textbf  {Vir1\_32\_Def} is the least expensive solution, mainly because \fig@textbf  {Bla1\_64\_Opt} has a much higher load cost. Also for the Ontoforce benchmark\fig@textbf  { Vir1\_32\_Opt} is the most cost-effective choice. The engine ranking is not conserved going from artificial to real-world benchmark.}}{20}{figure.11}}
\newlabel{fig:Fig11_AllSims_Correct}{{11}{20}{Benchmark Cost in \$ to load and execute 2000 queries in a stress test for WatDiv1000M or Ontoforce datasets for different setups. All stacked bars consists the load cost stacked on top of the runtime cost. Bar width encodes the amount of nodes. For WatDiv \textbf {Vir1\_32\_Def} is the least expensive solution, mainly because \textbf {Bla1\_64\_Opt} has a much higher load cost. Also for the Ontoforce benchmark\textbf { Vir1\_32\_Opt} is the most cost-effective choice. The engine ranking is not conserved going from artificial to real-world benchmark}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{20}{section.8}}
\bibstyle{bmc-mathphys}
\bibdata{bmc_article}
\bibcite{de2016big}{1}
\bibcite{alucc2014diversified}{2}
\bibcite{dewitte_swat4ls_2016}{3}
\bibcite{DBLP:journals/corr/abs-1103-5043}{4}
\bibcite{DBLP:conf/semweb/VerborghHMHVSCCMW14}{5}
\bibcite{graux2016multi}{6}
\bibcite{ladwig2011cumulusrdf}{7}
\bibcite{Schatzle:2016:SRQ:2977797.2977806}{8}
\bibcite{schatzle2011pigsparql}{9}
\bibcite{kotsevbenchmarking}{10}
\bibcite{hernandez2016querying}{11}
\bibcite{harris20094store}{12}
\bibcite{papailiou2013h}{13}
\bibcite{schatzle2014sempala}{14}
\thanksnewlabel{Athanks}{{1}{21}}
\thanksnewlabel{Bthanks}{{2}{21}}
\bibcite{rohloff2010high}{15}
\bibcite{Saleem}{16}
\bibcite{DBLP:conf/semweb/SchwarteHHSS11}{17}
\bibcite{gorlitz2011splendid}{18}
\bibcite{Acosta2011}{19}
\bibcite{saleem2014hibiscus}{20}
\bibcite{saleem2015feasible}{21}
\bibcite{boncz2005monetdb}{22}
\bibcite{neumann2010rdf}{23}
\bibcite{zou2011gstore}{24}
\bibcite{cudre2013nosql}{25}
\bibcite{khadilkar2012jena}{26}
\bibcite{wu2014biobenchmark}{27}
\bibcite{Schmidt2011}{28}
\bibcite{guo2005lubm}{29}
\bibcite{schmidt2009sp}{30}
\bibcite{bizer2009berlin}{31}
\bibcite{morsey2011dbpedia}{32}
\bibcite{DBLP:conf/sigmod/DuanKSU11}{33}
\bibcite{antezana2009cell}{34}
\bibcite{yamamoto2011allie}{35}
\bibcite{kinjo2011protein}{36}
\bibcite{uniprot2014uniprot}{37}
\bibcite{tateno2002dna}{38}
\bibcite{gorlitz2012splodge}{39}
\bibcite{ghemawat2003google}{40}
\bibcite{LDBC}{41}
\bibcite{HOBBIT}{42}
\bibcite{Boncz:2013:LBG:2513591.2527070}{43}
\bibcite{erling2015ldbc}{44}
\bibcite{Picalausa2011}{45}
\bibcite{Stegmaier_evaluationof}{46}
\bibcite{DBLP:journals/ws/FernandezMGPA13}{47}
\bibcite{saleem2016fine}{48}
\bibcite{Zeng}{49}
\bibcite{Harth}{50}
\bibcite{Ferre}{51}
\bibcite{Oren}{52}
\bibcite{Groth}{53}
\bibcite{cure2015evaluation}{54}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces {\mathversion  {bold}\bfseries  Sample figure title.} A short description of the figure content should go here.}}{23}{figure.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces {\mathversion  {bold}\bfseries  Sample figure title.} Figure legend text.}}{23}{figure.13}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Sample table title. This is where the description of the table should go.}}{23}{table.9}}
\newlabel{LastPage}{{}{23}{}{page.23}{}}
\xdef\lastpage@lastpage{23}
\xdef\lastpage@lastpageHy{23}
